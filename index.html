<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Multimodal RAG Survey">
    <meta name="keywords" content="multimodal rag, rag, language models, llm, mllm, retrieval-augmented generation, rag survey">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multimodal RAG Survey</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Ask in Any Modality:<br> A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
              <a href="https://aboots.github.io/">Mohammad Mahdi Abootorabi</a><sup>1</sup></span>
                            <span class="author-block">
              <a href="https://www.linkedin.com/in/amirhosein-zobeiri-81611826a/">Amirhosein Zobeiri</a><sup>3</sup></span>
                            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=qHOI1U0AAAAJ&view_op=list_works&sortby=pubdate">Mahdi Dehghani</a><sup>4</sup>,
            </span>
                            <span class="author-block">
              <a href="https://moalimkh.github.io/">Mohammadali Mohammadkhani</a><sup>2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://www.linkedin.com/in/bardia-mohammadi/">Bardia Mohammadi</a><sup>2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MMUouOIAAAAJ&hl=en">Omid Ghahroodi</a><sup>1</sup>,
              </span>
                            <span class="author-block">
                <a href="https://sharif.edu/~soleymani/">Mahdieh Soleymani Baghshah</a><sup>2</sup>,
                </span>
                            <span class="author-block">
                  <a href="https://www.ocf.berkeley.edu/~asgari/">Ehsaneddin Asgari</a><sup>1</sup>
                  </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Qatar Computing Research Institute,</span>
                            <span class="author-block"><sup>2</sup>Sharif University of Technology,</span>
                            <br>
                            <span class="author-block"><sup>3</sup>University of Tehran,</span>
                            <span class="author-block"><sup>4</sup>K.N. Toosi University of Technology</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                <a href="https://arxiv.org/abs/2502.08826"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                <a href="https://github.com/llm-lab-org/Multimodal-RAG-Survey"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                                <span>Repository</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="subtitle has-text-centered">
                    <span class="dnerf">Multimodal RAG</span> extends traditional Retrieval-Augmented Generation (RAG) frameworks by incorporating diverse data types, such as text, images, audio, and video, from external knowledge sources. This approach
                    aims to address AI limitations like hallucinations and outdated knowledge through the dynamic integration of this retrieved information, thereby improving the factual grounding, accuracy, and reasoning capabilities of the generated
                    outputs.
                </h2>
                <img src="static/images/MMRAG-pipeline.png" alt="BUFFET teaser.">
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and
                            updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal
                            alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG.
                            <br> <br> This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation.
                            We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support
                            advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3">Taxonomy of Recent Advances and Enhancements</h2>
                <img src="static/images/taxonomy.jpg" alt="BUFFET teaser.">
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3">Taxonomy of Application Domains</h2>
                <img src="static/images/application.jpg" alt="BUFFET teaser.">
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Centering columns -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Process and Innovations in Multimodal RAG</h2>
                    <div class="content has-text-justified">
                        <p>
                            The typical Multimodal RAG pipeline involves several key stages, alongside crucial innovations in training and robustness, to effectively integrate diverse data types for enhanced generation:
                        </p>
                        <ol>
                            <li>
                                <b>Multimodal Query and Corpus Encoding:</b> User queries and documents from a multimodal corpus (containing text, images, etc.) are processed by modality-specific encoders. These map the varied inputs into a shared semantic
                                space, enabling cross-modal comparison and alignment.
                            </li>
                            <li>
                                <b>Retrieval Strategy:</b> A retrieval model identifies relevant documents from the encoded corpus based on similarity to the query. This stage uses efficient search (like MIPS), modality-specific strategies (text-centric,
                                vision-centric, video-centric, document layout-aware), and often re-ranking or selection to prioritize the best candidates.
                            </li>
                            <li>
                                <b>Fusion Mechanisms:</b> The retrieved information, spanning multiple modalities, is effectively combined. Fusion mechanisms align and integrate these diverse data types using techniques like score fusion, attention-based
                                weighting of cross-modal interactions, or projecting inputs into unified representations.
                            </li>
                            <li>
                                <b>Augmentation Techniques:</b> Before generation, the retrieved context is refined. This can involve context enrichment (adding related info), adaptive retrieval (adjusting based on query complexity or feedback), or iterative
                                retrieval (refining results over multiple steps).
                            </li>
                            <li>
                                <b>Generation:</b> Finally, a Multimodal Large Language Model (MLLM) generates a response using the original query and the processed, retrieved multimodal context. Innovations include In-Context Learning with retrieved
                                examples, structured reasoning (like Chain-of-Thought), instruction tuning for specific tasks, and ensuring source attribution.
                            </li>
                            <li>
                                <b>Training Strategies:</b> Training involves multistage processes, often starting with pretraining on large paired datasets to learn cross-modal relationships (e.g., using contrastive loss like InfoNCE or alignment losses)
                                and followed by fine-tuning on specific tasks using objectives like cross-entropy for generation or specialized losses for alignment and disentanglement.
                            </li>
                            <li>
                                <b>Robustness and Noise Management:</b> Techniques are employed to handle challenges like noisy retrieval inputs and modality-specific biases. Methods include injecting irrelevant results or noise during training, using
                                knowledge distillation, employing adaptive knowledge selection strategies, or applying regularization techniques (like Query Dropout) to enhance model resilience and focus.
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Centering columns -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Open Problems and Future Directions</h2>
                    <div class="content has-text-justified">
                        <p>
                            Key challenges and future research opportunities in Multimodal RAG include:
                        </p>
                        <ul>
                            <li>
                                <b>Improving Generalization, Explainability, and Robustness:</b> Addressing domain adaptation issues, modality biases (like over-reliance on text), the lack of precise source attribution (especially for non-textual data),
                                and vulnerability to adversarial inputs or low-quality sources.
                            </li>
                            <li>
                                <b>Enhancing Reasoning, Alignment, and Retrieval:</b> Developing better compositional reasoning across modalities, improving entity-aware retrieval, mitigating retrieval biases (e.g., position sensitivity, redundancy),
                                exploring knowledge graphs, and creating truly unified embedding spaces for direct multimodal search.
                            </li>
                            <li>
                                <b>Developing Agent-Based and Self-Guided Systems:</b> Moving towards interactive, agentic systems that use feedback (like reinforcement learning or human alignment) to self-assess retrieval needs, evaluate relevance, dynamically
                                choose modalities, and refine outputs iteratively, achieving robust "any-to-any" modality support.
                            </li>
                            <li>
                                <b>Integrating with Real-World Data and Embodied AI:</b> Incorporating diverse real-world sensor data alongside traditional modalities to enhance situational awareness, aligning with trends towards embodied AI for applications
                                in robotics, navigation, and physics-informed reasoning.
                            </li>
                            <li>
                                <b>Addressing Long-Context, Efficiency, Scalability, and Personalization:</b> Overcoming computational bottlenecks in processing long videos or multi-page documents, optimizing the speed-accuracy trade-off for efficiency
                                and scalability (especially for edge devices), exploring user-specific personalization while ensuring privacy, and creating better datasets for evaluating complex reasoning and robustness.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{abootorabi2025askmodalitycomprehensivesurvey,
                title={Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation}, 
                author={Mohammad Mahdi Abootorabi and Amirhosein Zobeiri and Mahdi Dehghani and Mohammadali Mohammadkhani and Bardia Mohammadi and Omid Ghahroodi and Mahdieh Soleymani Baghshah and Ehsaneddin Asgari},
                year={2025},
                eprint={2502.08826},
                archivePrefix={arXiv},
                primaryClass={cs.CL},
                url={https://arxiv.org/abs/2502.08826}, 
        }</code></pre>
        </div>
    </section>



    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <!-- <a class="icon-link" href="https://akariasai.github.io/" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a> -->
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, we just ask that you link back to this page in the footer. Please remember to remove the analytics
                            code included in the header of the website which you do not want on your website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>