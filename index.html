<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval">
    <meta name="keywords" content="Multimodal IR, Speech Retrieval, Contrastive Learning, multimodal retrieval">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CLASP</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
              <a href="https://aboots.github.io/">Mohammad Mahdi Abootorabi</a><sup>1</sup> and </span>
                            <span class="author-block">
                  <a href="https://www.ocf.berkeley.edu/~asgari/">Ehsaneddin Asgari</a><sup>1</sup>
                  </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Qatar Computing Research Institute</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                <a href="https://arxiv.org/abs/2412.13071"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-88717-8_2"
                                       class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        <i class="fa fa-clipboard"></i>
                                </span>
                                <span>Springer Nature</span>
                                </a>


                                <span class="link-block">
                                    <a href="https://dl.acm.org/doi/10.1007/978-3-031-88717-8_2"
                                       class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        <i class="fa fa-clipboard"></i>
                                </span>
                                <span>ACM Digital Library</span>
                                </a>
                                <br>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                <a href="https://huggingface.co/llm-lab/CLASP"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <span style="font-size: 24px;">ðŸ¤—</span>
                                </span>
                                <span>Models</span>
                                </a>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                <a href="https://huggingface.co/datasets/llm-lab/SpeechBrown"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                                <span>Speech Brown Dataset</span>
                                </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                <a href="https://github.com/language-modeling-lab/CLASP"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                                <span>Code</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="subtitle has-text-centered">
                    <span class="dnerf" style="color: red">CLASP</span> is a novel lightweight multilingual multimodal representation model designed for audio-text retrieval. It is capable of generating rich multilingual semantic embeddings for sentence-level
                    audio that can be used in different text-speech tasks.
                </h2>
                <figure class="has-text-centered mt-5">
                    <img src="static/images/CLASP_panel.png" alt="Overview of the two proposed strategies for the fusion encoder architecture.">
                    <figcaption class="mt-2">CLASP training and evaluation pipeline.</figcaption>
                </figure>
                <hr>
                <!-- Added figure with image and caption -->
                <figure class="has-text-centered mt-5">
                    <img src="static/images/CLASP_core_endoder.png" alt="Overview of the two proposed strategies for the fusion encoder architecture.">
                    <figcaption class="mt-2">Overview of the two proposed strategies for the fusion encoder architecture.</figcaption>
                </figure>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During training, we utilize
                            our newly introduced speech-text dataset, which encompasses 15 diverse categories ranging from fiction to religion. CLASP's audio component integrates audio spectrograms with a pre-trained self-supervised speech model, while
                            its language encoding counterpart employs a sentence encoder pre-trained on over 100 languages. This unified lightweight model bridges the gap between various modalities and languages, enhancing its effectiveness in handling
                            and retrieving multilingual and multimodal data. Our evaluations across multiple languages demonstrate that CLASP establishes new benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional ASR-based retrieval methods
                            that rely on transcribing speech into text for subsequent text retrieval, especially in specific scenarios.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Centering columns -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Summary</h2>
                    <div class="content has-text-justified">
                        <ul>
                            <li>
                                We introduce CLASP (Contrastive Language-Speech Pretraining), a novel lightweight multilingual, multimodal representation designed for audio-text retrieval.
                            </li>
                            <li>
                                We introduce a diverse paired speech-text dataset (Speech Brown) in 15 categories, encompassing a wide range of topics from fiction to religion.
                            </li>
                            <li>
                                We show that the combination of audio spectrograms with a pre-trained self-supervised speech model improves audio encoding in retrieval applications.
                            </li>
                            <li>
                                Evaluations in multiple languages demonstrate that CLASP sets new benchmarks in HITS@1, Mean Reciprocal Rank (MRR), and Mean Rank (meanR) metrics.
                            </li>
                            <li>
                                CLASP Leverages audio spectrograms in addition to self-supervised speech encoding in a contrastive learning framework to enhance semantic representation.
                            </li>
                            <li>
                                CLASP is Simpler, faster, and more size-efficient than ASR-based retrieval pipelines.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Centering columns -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Speech Brown Dataset</h2>

                    <!-- Add figure with image and caption here -->
                    <figure class="has-text-centered mb-5">
                        <img src="static/images/CLASP_data.png" alt="Speech Brown dataset synthesis pipeline" style="max-width: 100%;">
                        <figcaption class="mt-2">Speech Brown dataset synthesis pipeline.</figcaption>
                    </figure>

                    <div class="content has-text-justified">
                        <ul>
                            <li> The Speech Brown Dataset is a comprehensive speech-text paired corpus spanning 15 diverse categories including science fiction, religion, romance, and news, providing rich contextual variety for speech processing research.</li>
                            <li> Comprising over 55,000 sentence-level samples synthesized using the NVIDIA Tacotron 2 text-to-speech model, this dataset offers high-quality audio-text pairs for multimodal learning applications. </li>
                            <li> With approximately 30 GB of data, the dataset features an average of 19 tokens and 96.72 characters per sample, making it ideal for both short-form speech recognition and text-to-speech development. </li>
                            <li> Each sample is meticulously categorized across domains like adventure, belles_lettres, editorial, government, hobbies, humor, learned, and more, enabling domain-specific speech processing research. </li>
                            <li> The dataset's balanced representation across multiple genres provides researchers with a robust foundation for developing and evaluating speech-text retrieval systems and multimodal language models. </li>
                            <li> Built upon the renowned Brown corpus, this speech-enhanced version extends its utility to modern speech processing applications while maintaining the linguistic diversity of the original collection. </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Centering columns -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results and Analysis</h2>
                    <figure class="has-text-centered mb-5">
                        <img src="static/images/clasp_table.png" alt="Speech Brown dataset synthesis pipeline" style="max-width: 100%;">
                    </figure>

                    <div class="content has-text-justified">
                        <ul>
                            <li>CLASP outperforms Wav2Vec2 and nearly matches HuBERT in retrieval metrics, while reducing model size by approximately 50% and improving inference speed by around 10%, demonstrating superior efficiency without sacrificing performance.</li>
                            <li>With significantly lower meanR values (7.71) compared to HuBERT (17.84) and Wav2Vec2 (38.3), CLASP demonstrates better retrieval ranking and enhanced handling of outliers, leading to more accurate and reliable search results.</li>
                            <li>Despite being trained on only ~130 hours of dataâ€”far less than HuBERT's 60,000+ hoursâ€”CLASP achieves comparable performance with less than 1.5% drop in HITS@1 and only ~0.8% reduction in MRR, showcasing remarkable data efficiency.</li>
                            <li>The integration of spectrogram analysis with self-supervised speech encoding boosts HITS@1 by approximately 3%, highlighting the benefit of abstract feature extraction from spectrogram waveforms for semantic understanding.</li>
                            <li>Our experiments reveal that the combination of LaBSE text encoder with HuBERT speech embeddings using the concatenation fusion strategy outperforms alternative architectures, achieving the highest retrieval scores across evaluation
                                metrics.
                            </li>
                            <li>CLASP demonstrates exceptional multilingual capability with impressive scores across diverse languages.
                            </li>
                            <li>Our contrastive learning approach enables CLASP to capture nuanced semantic relationships by learning from both positive matches and negative samples, resulting in more robust and discriminative representations for audio-text
                                retrieval.
                            </li>
                        </ul>
                    </div>

                    <!-- Add t-SNE visualization figure -->
                    <figure class="has-text-centered mt-5">
                        <img src="static/images/tsne_plot.png" alt="t-SNE visualization of sentence embeddings" style="max-width: 100%;">
                        <figcaption class="mt-2">t-SNE visualization of sentence embeddings across modalities, demonstrating effective projection into a shared representation space for the test dataset.</figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>



    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@inproceedings{10.1007/978-3-031-88717-8_2,
                author = {Abootorabi, Mohammad Mahdi and Asgari, Ehsaneddin},
                title = {CLASP: Contrastive Language-Speech Pretraining for&nbsp;Multilingual Multimodal Information Retrieval},
                year = {2025},
                isbn = {978-3-031-88716-1},
                publisher = {Springer-Verlag},
                address = {Berlin, Heidelberg},
                url = {https://doi.org/10.1007/978-3-031-88717-8_2},
                doi = {10.1007/978-3-031-88717-8_2},
                abstract = {This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During training, we utilize our newly introduced speech-text dataset, which encompasses 15 diverse categories ranging from fiction to religion. CLASPâ€™s audio component integrates audio spectrograms with a pre-trained self-supervised speech model, while its language encoding counterpart employs a sentence encoder pre-trained on over 100 languages. This unified lightweight model bridges the gap between various modalities and languages, enhancing its effectiveness in handling and retrieving multilingual and multimodal data. Our evaluations across multiple languages demonstrate that CLASP establishes new benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional ASR-based retrieval methods that rely on transcribing speech into text for subsequent text retrieval, especially in specific scenarios.},
                booktitle = {Advances in Information Retrieval: 47th European Conference on Information Retrieval, ECIR 2025, Lucca, Italy, April 6â€“10, 2025, Proceedings, Part IV},
                pages = {10â€“20},
                numpages = {11},
                keywords = {Multimodal IR, Speech Retrieval, Contrastive Learning},
                location = {Lucca, Italy}
                }</code></pre>
        </div>
    </section>



    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <!-- <a class="icon-link" href="https://akariasai.github.io/" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a> -->
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, we just ask that you link back to this page in the footer. Please remember to remove the analytics
                            code included in the header of the website which you do not want on your website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>